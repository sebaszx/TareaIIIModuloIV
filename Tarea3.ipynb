{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItzpTu3clRI5"
      },
      "source": [
        "# Tarea 3\n",
        "Estudiante: Sebastián Porras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5SuTdqVlLpV"
      },
      "source": [
        "Dataset : \n",
        "\n",
        "Breast-Cancer.csv\n",
        "\n",
        "El cáncer de mama es el cáncer más común entre las mujeres en el mundo. Representa el 25 % de todos los casos de cáncer y afectó a más de 2,1 millones de personas solo en 2015. Comienza cuando las células en el seno comienzan a crecer sin control. Estas células generalmente forman tumores que se pueden ver a través de rayos X o sentir como bultos en el área del seno.\n",
        "\n",
        "**La varaible a clasificar será Diagnosis (M - Maligno y  B - Benigno)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zL0JFEVlLpS"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyCI96tDlLpU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,DateType,FloatType\n",
        "from pyspark.sql.functions import col,isnan, when, count,monotonically_increasing_id\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier,DecisionTreeClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize']=(15,15)\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VFdqmydlLpU"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Tarea3\") \\\n",
        "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.2.14.jar\") \\\n",
        "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.2.14.jar\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtSjFcW2lLpV"
      },
      "outputs": [],
      "source": [
        "def EscribirDatosEnTabla(dataframe,NombreTabla):\n",
        "    dataframe \\\n",
        "    .write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .mode('overwrite') \\\n",
        "    .option(\"url\", \"jdbc:postgresql://host.docker.internal:5433/postgres\") \\\n",
        "    .option(\"user\", \"postgres\") \\\n",
        "    .option(\"password\", \"testPassword\") \\\n",
        "    .option(\"dbtable\", NombreTabla) \\\n",
        "    .save()\n",
        "\n",
        "def LeerDatosEnBD(NombreTabla):\n",
        "    return spark \\\n",
        "    .read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://host.docker.internal:5433/postgres\") \\\n",
        "    .option(\"user\", \"postgres\") \\\n",
        "    .option(\"password\", \"testPassword\") \\\n",
        "    .option(\"dbtable\", NombreTabla) \\\n",
        "    .load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy1ovLcGllSi"
      },
      "source": [
        "#Carga de Datos\n",
        "\n",
        "Se cargan los datos bajo el schema creado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N-TVb9IlLpc"
      },
      "outputs": [],
      "source": [
        "data_schema = StructType([\n",
        "                StructField(\"id\",IntegerType()),\n",
        "                StructField(\"diagnosis\",StringType()),\n",
        "                StructField(\"radius_mean\",FloatType()),\n",
        "                StructField(\"texture_mean\",FloatType()),\n",
        "                StructField(\"perimeter_mean\",FloatType()),\n",
        "                StructField(\"area_mean\",FloatType()),\n",
        "                StructField(\"smoothness_mean\",FloatType()),\n",
        "                StructField(\"compactness_mean\",FloatType()),\n",
        "                StructField(\"concavity_mean\",FloatType()),\n",
        "                StructField(\"concave points_mean\",FloatType()),\n",
        "                StructField(\"symmetry_mean\",FloatType()),\n",
        "                StructField(\"fractal_dimension_mean\",FloatType()),\n",
        "                StructField(\"radius_se\",FloatType()),\n",
        "                StructField(\"texture_se\",FloatType()),\n",
        "                StructField(\"perimeter_se\",FloatType()),\n",
        "                StructField(\"area_se\",FloatType()),\n",
        "                StructField(\"smoothness_se\",FloatType()),\n",
        "                StructField(\"compactness_se\",FloatType()),\n",
        "                StructField(\"concavity_se\",FloatType()),\n",
        "                StructField(\"concave points_se\",FloatType()),\n",
        "                StructField(\"symmetry_se\",FloatType()),\n",
        "                StructField(\"fractal_dimension_se\",FloatType()),\n",
        "                StructField(\"radius_worst\",FloatType()),\n",
        "                StructField(\"texture_worst\",FloatType()),\n",
        "                StructField(\"perimeter_worst\",FloatType()),\n",
        "                StructField(\"area_worst\",FloatType()),\n",
        "                StructField(\"smoothness_worst\",FloatType()),\n",
        "                StructField(\"compactness_worst\",FloatType()),\n",
        "                StructField(\"concavity_worst\",FloatType()),\n",
        "                StructField(\"concave points_worst\",FloatType()),\n",
        "                StructField(\"symmetry_worst\",FloatType()),\n",
        "                StructField(\"fractal_dimension_worst\",FloatType())]\n",
        ")\n",
        "            \n",
        "data = spark.read.csv(\n",
        "    'breast-cancer.csv',\n",
        "    sep = ',',\n",
        "    header = True,\n",
        "    schema = data_schema\n",
        "    )\n",
        "\n",
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StTPcbLqlLpd"
      },
      "outputs": [],
      "source": [
        "data.show(n=10,truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj_L1I7vlLpe"
      },
      "source": [
        "#Limpieza de datos\n",
        "\n",
        "Contamos los valores nulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7aep1eblLpe"
      },
      "outputs": [],
      "source": [
        "\n",
        "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]\n",
        "   ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gEfS1G-lLpe"
      },
      "source": [
        "Se hace un encoder para la columna diagnosis ya que en estos momentos es una variable categórica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3ZJUK0DlLpf"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"diagnosis\", outputCol=\"diagnosisIndex\") \n",
        "indexed = indexer.fit(data).transform(data) \n",
        "indexed.select(\"diagnosis\",\"diagnosisIndex\").show()\n",
        "data=indexed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiO-v1mhlLpf"
      },
      "source": [
        "# Creación de graficas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9hN3q9klLpf"
      },
      "outputs": [],
      "source": [
        "y=data.toPandas()[\"radius_mean\"].values.tolist()\n",
        "x = np.arange(0, len(y))\n",
        "print(len(y))\n",
        "plt.title(\"radio medio del cancer\")\n",
        "\n",
        "plt.plot(x, y, color =\"green\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG39I_SylLpg"
      },
      "outputs": [],
      "source": [
        "y=data.toPandas()[\"symmetry_mean\"].values.tolist()\n",
        "x = np.arange(0, len(y))\n",
        "print(len(y))\n",
        "plt.title(\"simetria media del cancer\")\n",
        "\n",
        "plt.plot(x, y, color =\"green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPWE4hL8lLpg"
      },
      "outputs": [],
      "source": [
        "y=data.toPandas()[\"smoothness_mean\"].values.tolist()\n",
        "x = np.arange(0, len(y))\n",
        "print(len(y))\n",
        "plt.title(\"suavidad media del cancer\")\n",
        "\n",
        "plt.plot(x, y, color =\"green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn_H16qrlLpg"
      },
      "outputs": [],
      "source": [
        "y=data.toPandas()[\"compactness_se\"].values.tolist()\n",
        "x = np.arange(0, len(y))\n",
        "print(len(y))\n",
        "plt.title(\"compacidad del cancer\")\n",
        "\n",
        "plt.plot(x, y, color =\"green\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtUf9XwvlLpg"
      },
      "outputs": [],
      "source": [
        "x=data.toPandas()[\"diagnosis\"].values.tolist()\n",
        "\n",
        "pd.Series(x).value_counts(sort=False).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEwSpd98lLpg"
      },
      "source": [
        "De las columnas analizadas ninguna parece seguir una distribución clara"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja1aSXCglLpg"
      },
      "source": [
        "# Vectorización\n",
        "\n",
        "Creamos vectores para poder crear la correlacion "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlGYsY9XlLph"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "def  vectorizar(dataframe):\n",
        "    assembler = VectorAssembler(\n",
        "                    inputCols=[\n",
        "                \"radius_mean\",\n",
        "                \"texture_mean\",\n",
        "                \"perimeter_mean\",\n",
        "                \"area_mean\",\n",
        "                \"smoothness_mean\",\n",
        "                \"compactness_mean\",\n",
        "                \"concavity_mean\",\n",
        "                \"concave points_mean\",\n",
        "                \"symmetry_mean\",\n",
        "                \"fractal_dimension_mean\",\n",
        "                \"radius_se\",\n",
        "                \"texture_se\",\n",
        "                \"perimeter_se\",\n",
        "                \"area_se\",\n",
        "                \"smoothness_se\",\n",
        "                \"compactness_se\",\n",
        "                \"concavity_se\",\n",
        "                \"concave points_se\",\n",
        "                \"symmetry_se\",\n",
        "                \"fractal_dimension_se\",\n",
        "                \"radius_worst\",\n",
        "                \"texture_worst\",\n",
        "                \"perimeter_worst\",\n",
        "                \"area_worst\",\n",
        "                \"smoothness_worst\",\n",
        "                \"compactness_worst\",\n",
        "                \"concavity_worst\",\n",
        "                \"concave points_worst\",\n",
        "                \"symmetry_worst\",\n",
        "                \"fractal_dimension_worst\"],\n",
        "                    outputCol='features')\n",
        "\n",
        "    vector_df = assembler.transform(dataframe)\n",
        "    vector_df = vector_df.select(['features', 'diagnosisIndex'])\n",
        "    return vector_df\n",
        "vector_df=vectorizar(data)\n",
        "\n",
        "pearson_matrix = Correlation.corr(vector_df, 'features').collect()[0][0]\n",
        "\n",
        "sns.heatmap(pearson_matrix.toArray(), annot=True, fmt=\".2f\", cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE7ObOHQlLph"
      },
      "source": [
        "# Estandarización\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejreIb3jlLph"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "def Escalador(dataframe):\n",
        "    standard_scaler = StandardScaler(inputCol='features', outputCol='scaled')\n",
        "    scale_model = standard_scaler.fit(dataframe)\n",
        "\n",
        "    scaled_df = scale_model.transform(dataframe)\n",
        "   \n",
        "    return scaled_df\n",
        "scaled_df=Escalador(vector_df)\n",
        "scaled_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur3l_rNHlLph"
      },
      "source": [
        "Escritura a la BD con los datos limpios sin vecotorizar ni escalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e6ykt-3lLpi"
      },
      "outputs": [],
      "source": [
        "EscribirDatosEnTabla(data,\"tarea3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1JzKMzmlLpi"
      },
      "source": [
        "Lectura de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX5QrFRXlLpi"
      },
      "outputs": [],
      "source": [
        "data=LeerDatosEnBD(\"tarea3\")\n",
        "data.show()\n",
        "data=data.drop(\"id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0QNtkt7pAV-"
      },
      "source": [
        "Separación de datos en train y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7GVv5bxlLpi"
      },
      "outputs": [],
      "source": [
        "train, test = data.randomSplit([0.7, 0.3], seed = 2)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))\n",
        "train.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PMeg-UMpGLy"
      },
      "source": [
        "# Entranamiento de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUGJAwL4lLpj"
      },
      "outputs": [],
      "source": [
        "train.printSchema()\n",
        "train=vectorizar(train)\n",
        "\n",
        "train=Escalador(train)\n",
        "\n",
        "testData=vectorizar(test)\n",
        "testData=Escalador(testData)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h281vx6QlLpi"
      },
      "source": [
        " ## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6LeosP2lLpi"
      },
      "source": [
        "### Sin usar CrossValidation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvm-dlBDlLpj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol = 'scaled', labelCol = 'diagnosisIndex', numTrees=15)\n",
        "rfModel = rf.fit(train)\n",
        "predictions = rfModel.transform(testData)\n",
        "\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"diagnosisIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "predictions.printSchema()\n",
        "predictions=predictions.withColumnRenamed(\"prediction\",\"Prediccion\")\n",
        "predictions.select(\"diagnosisIndex\", \"Prediccion\").show(30)\n",
        "predictions=predictions.select(\"Prediccion\")\n",
        "print(accuracy)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oDjE1oXlLpj"
      },
      "source": [
        "### Usando CrossValidation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PXFzsiblLpj"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(featuresCol = 'scaled', labelCol = 'diagnosisIndex')\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"diagnosisIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "rfparamGrid = (ParamGridBuilder()\n",
        "             #.addGrid(rf.maxDepth, [2, 5, 10, 20, 30])\n",
        "               .addGrid(rf.maxDepth, [2, 5, 10])\n",
        "             #.addGrid(rf.maxBins, [10, 20, 40, 80, 100])\n",
        "               .addGrid(rf.maxBins, [5, 10, 20])\n",
        "             #.addGrid(rf.numTrees, [5, 20, 50, 100, 500])\n",
        "               .addGrid(rf.numTrees, [5, 20, 50])\n",
        "             .build())\n",
        "\n",
        "rfcv = CrossValidator(estimator = rf,\n",
        "                      estimatorParamMaps = rfparamGrid,\n",
        "                      evaluator = evaluator,\n",
        "                      numFolds = 5)\n",
        "\n",
        "rfModel = rfcv.fit(train)\n",
        "\n",
        "predictions = rfModel.transform(testData)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "predictions.printSchema()\n",
        "predictions=predictions.withColumnRenamed(\"prediction\",\"Prediccion\")\n",
        "predictions.select(\"diagnosisIndex\", \"Prediccion\").show(30)\n",
        "predictions=predictions.select(\"Prediccion\")\n",
        "print(accuracy)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lbKCWRFlLpj"
      },
      "source": [
        "### Escritura del modelo 1 a postgres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrQssfqvlLpj"
      },
      "outputs": [],
      "source": [
        "\n",
        "results=test.withColumn(\"mid\",monotonically_increasing_id()).\\\n",
        "join(predictions.withColumn(\"mid\",monotonically_increasing_id()),[\"mid\"]).\\\n",
        "drop(\"mid\")\n",
        "results.printSchema()\n",
        "results.show(3)\n",
        "EscribirDatosEnTabla(results,\"modelo1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVrvuslxlLpk"
      },
      "source": [
        "\n",
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gKD9Sa2lLpk"
      },
      "source": [
        "### Sin usar CrossValidation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7G-YhTslLpk"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(featuresCol = 'scaled', labelCol = 'diagnosisIndex',  maxDepth=15)\n",
        "dtModel = dt.fit(train)\n",
        "predictions = dtModel.transform(testData)\n",
        "\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"diagnosisIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "predictions.printSchema()\n",
        "predictions=predictions.withColumnRenamed(\"prediction\",\"Prediccion\")\n",
        "predictions.select(\"diagnosisIndex\", \"Prediccion\").show(30)\n",
        "predictions=predictions.select(\"Prediccion\") \n",
        "\n",
        "print(accuracy)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSHfbflRlLpk"
      },
      "source": [
        "### Usando CrossValidation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ZrKbEAIklLpl",
        "outputId": "8edd782d-59a4-4696-dd40-cdca35c5ec92"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2ac3bdffeabd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'scaled'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'diagnosisIndex'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmaxDepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m dtparamGrid = (ParamGridBuilder()\n\u001b[1;32m      3\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxDepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxBins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              .build())\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
          ]
        }
      ],
      "source": [
        "dt = DecisionTreeClassifier(featuresCol = 'scaled', labelCol = 'diagnosisIndex',  maxDepth=15)\n",
        "dtparamGrid = (ParamGridBuilder()\n",
        "             .addGrid(dt.maxDepth, [2, 5, 10, 20, 30])\n",
        "             .addGrid(dt.maxBins, [10, 20, 40, 80, 100])\n",
        "             .build())\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"diagnosisIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "dtcv = CrossValidator(estimator = dt,\n",
        "                      estimatorParamMaps = dtparamGrid,\n",
        "                      evaluator = evaluator,\n",
        "                      numFolds = 5)   \n",
        "dtcvModel = dtcv.fit(train)\n",
        "print(dtcvModel)\n",
        "\n",
        "predictions = dtcvModel.transform(testData)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "predictions.printSchema()\n",
        "predictions=predictions.withColumnRenamed(\"prediction\",\"Prediccion\")\n",
        "predictions.select(\"diagnosisIndex\", \"Prediccion\").show(30)\n",
        "predictions=predictions.select(\"Prediccion\")\n",
        "print(accuracy)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4IdYxQnlLpl"
      },
      "source": [
        "### Escritura del modelo 2 a postgres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xa5BOd7lLpl"
      },
      "outputs": [],
      "source": [
        "results=test.withColumn(\"mid\",monotonically_increasing_id()).\\\n",
        "join(predictions.withColumn(\"mid\",monotonically_increasing_id()),[\"mid\"]).\\\n",
        "drop(\"mid\")\n",
        "results.printSchema()\n",
        "results.show(3)\n",
        "EscribirDatosEnTabla(results,\"modelo2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z6MqXcalLpl"
      },
      "source": [
        "# Analisis de resultados\n",
        "\n",
        "Los algoritmos utilizados fueron Random Forest y Decision Trees.\n",
        "Con cada uno de estos se hicieron dos pruebas, una utilizando cross validation y la otra con el algoritmo vanilla.\n",
        "\n",
        "* Random Forest Vanilla: 0.935672514619883 Accuracy\n",
        "* Random Forest Crossvalidation : 0.9239766081871345 Accuracy\n",
        "* Decision Tree Vanilla: 0.935672514619883 Accuracy\n",
        "* Decision Tree CrossValidation: 0.9122807017543859 Accuracy\n",
        "\n",
        "Es interesante como utilizando cross validation en ambos algoritmos el resultado nos da un poco peor que con solo usar el algoritmo vanilla.  Podemos concluir que sin importar el algoritmo que escogimos la versión vanilla da un 93.5% de accuracy "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
